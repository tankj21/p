import requests
from bs4 import BeautifulSoup
import datetime
import os
import csv

# ========= è¨­å®š =========
USERNAME = "search?f=tweets&q=pon2325_vrc"  # â†ãƒªãƒ—ãƒ©ã‚¤å«ã‚ãŸå…¨æŠ•ç¨¿ã‚’å¯¾è±¡ã«ã™ã‚‹
NITTER_BASE = "https://nitter.net"  # ãƒŸãƒ©ãƒ¼ã«å¤‰ãˆã¦ã‚‚OK
WEBHOOK_URL = os.getenv("WEBHOOK_URL")  # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰Webhookã‚’å–å¾—
CSV_FILE = "sent_tweets.csv"  # é€ä¿¡æ¸ˆã¿è¨˜éŒ²ç”¨ãƒ•ã‚¡ã‚¤ãƒ«

# ========= é€ä¿¡æ¸ˆã¿ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿ =========
def load_sent_tweets():
    if not os.path.exists(CSV_FILE):
        return set()
    with open(CSV_FILE, newline="", encoding="utf-8") as f:
        return set(row[0] for row in csv.reader(f))

# ========= æ–°ã—ãé€ä¿¡ã—ãŸURLã‚’ä¿å­˜ =========
def save_sent_tweet(tweet_link):
    with open(CSV_FILE, "a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([tweet_link])

# ========= ãƒ„ã‚¤ãƒ¼ãƒˆå–å¾—ï¼ˆæœ€å¤§ä»¶æ•°ã‚’æŒ‡å®šï¼‰ =========
def fetch_latest_tweets(limit=10):
    try:
        url = f"{NITTER_BASE}/{USERNAME}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
        }
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()

        soup = BeautifulSoup(res.text, "html.parser")
        tweet_items = soup.select(".timeline-item")[:limit]

        tweets = []
        for tweet in tweet_items:
            content = tweet.select_one(".tweet-content")
            time_tag = tweet.select_one(".tweet-date a")
            image_tag = tweet.select_one(".attachment.image > a[href$='.jpg'], .attachment.image > a[href$='.png']")

            if not content or not time_tag:
                continue  # ã‚¹ã‚­ãƒƒãƒ—

            tweet_data = {
                "content": content.get_text(strip=True),
                "link": f"{NITTER_BASE}{time_tag['href']}",
                "image": f"{NITTER_BASE}{image_tag['href']}" if image_tag else None
            }
            tweets.append(tweet_data)

        return tweets
    except Exception as e:
        print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        return []

# ========= Discordã¸é€ä¿¡ =========
def send_to_discord(tweet):
    if not WEBHOOK_URL:
        print("âŒ WEBHOOK_URL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    embed = {
        "title": "ğŸ“¢ æ–°ã—ã„æŠ•ç¨¿ï¼",
        "description": tweet["content"],
        "url": tweet["link"],
        "color": 0x1DA1F2,
        "timestamp": datetime.datetime.utcnow().isoformat()
    }

    if tweet["image"]:
        embed["image"] = {"url": tweet["image"]}

    payload = {
        "username": "Xé€šçŸ¥Bot",
        "embeds": [embed]
    }

    res = requests.post(WEBHOOK_URL, json=payload)
    if res.status_code == 204:
        print(f"âœ… é€ä¿¡æ¸ˆã¿: {tweet['link']}")
        save_sent_tweet(tweet["link"])
    else:
        print(f"âŒ é€ä¿¡å¤±æ•—: {res.status_code} - {res.text}")

# ========= å®Ÿè¡Œ =========
if __name__ == "__main__":
    tweets = fetch_latest_tweets(limit=10)  # â†å–å¾—ä»¶æ•°ã¯å¿…è¦ã«å¿œã˜ã¦å¢—ã‚„ã—ã¦OK
    sent_links = load_sent_tweets()

    new_tweets = [tweet for tweet in tweets if tweet["link"] not in sent_links]

    if not new_tweets:
        print("â­ï¸ æ–°ã—ã„ãƒ„ã‚¤ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    else:
        for tweet in reversed(new_tweets):  # å¤ã„é †ã«é€ä¿¡ï¼ˆæ™‚ç³»åˆ—é †ã§è¦‹ã‚„ã™ãï¼‰
            send_to_discord(tweet)
